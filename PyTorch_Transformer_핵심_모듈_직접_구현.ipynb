{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hx-q0KQY-IQF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Positional Encoding (위치 정보 인코딩)**"
      ],
      "metadata": {
        "id": "YVFLRGRMAqsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 사인·코사인 방식의 위치 인코딩\n",
        "\n",
        "#### **개념**:\n",
        "  - 트랜스포머는 시퀀스의 순서 정보를 학습하지 못하므로 위치 정보를 추가로 제공\n",
        "  - PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "  - PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))"
      ],
      "metadata": {
        "id": "XMImqu_KBAEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math # numpy 대신 math나 torch 사용 권장\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # 위치 인코딩 행렬 생성 (최대시퀀스길이, 차원)\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "\n",
        "        # 위치 인덱스\n",
        "        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # 주파수 계산: 10000^(2i/d_model)\n",
        "        # np.log 대신 torch.log를 사용하거나 math.log를 사용합니다.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # 짝수 인덱스: sin\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "\n",
        "        # 홀수 인덱스: cos\n",
        "        if d_model % 2 == 1:\n",
        "            pe[:, 1::2] = torch.cos(pos * div_term[:-1])\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "\n",
        "        # 버퍼로 등록 (배치 차원 추가: 1, max_seq_len, d_model)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        이 함수는 반드시 __init__과 세로 줄이 맞춰져 있어야 합니다.\n",
        "        x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # x의 길이에 맞춰서 위치 정보를 더해줌\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ],
      "metadata": {
        "id": "PDXM_ErDA0QQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Self-Attention**"
      ],
      "metadata": {
        "id": "yJCOjTTvEq82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단일 헤드의 Self-Attention 메커니즘\n",
        "    \n",
        "  #### **개념**:\n",
        "  - **Query(Q)**: 각 토큰이 어떤 토큰을 봐야 하는지 나타냄\n",
        "  - **Key(K)**: 각 토큰이 찾아질 수 있는 특성을 나타냄\n",
        "  - **Value(V)**: 실제 정보를 담음"
      ],
      "metadata": {
        "id": "tGeS1edREvTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **수식:**\n",
        "<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSRyzcV771Etb3fBSPSFzW_hBZT1H_h9DvgrQ&s' height=120>"
      ],
      "metadata": {
        "id": "r9-FZEW9FK5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, d_model, d_k=None):\n",
        "    super().__init__()\n",
        "    # d_model: 임베딩 차원 (예: 256)\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # d_k: 각 헤드의 차원 (명시하지 않으면 d_model과 같음)\n",
        "    self.d_k = d_k if d_k is not None else d_model\n",
        "\n",
        "    # Q, K, V 선형 변환\n",
        "    # 입력 d_model 차원 → 출력 d_k 차원\n",
        "    # 예: (배치, 시퀀스, 256) → (배치, 시퀀스, 64)\n",
        "    self.linear_q = nn.Linear(d_model, self.d_k)\n",
        "    self.linear_k = nn.Linear(d_model, self.d_k)\n",
        "    self.linear_v = nn.Linear(d_model, self.d_k)\n",
        "\n",
        "    # QK^T을 d_k로 나눠서 너무 크지 않도록 정규화\n",
        "    # softmax가 극단값(0 또는 1)이 되지 않도록 방지\n",
        "    self.scale = np.sqrt(self.d_k)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    # 1. 선형변환으로 Q, K, V 생성\n",
        "    # 예: (32, 20, 256) → (32, 20, 64)\n",
        "    Q = self.linear_q(query)  # Query: 무엇을 찾을 것인가\n",
        "    K = self.linear_k(key)    # Key: 여기 찾을 수 있는 특성이 있나\n",
        "    V = self.linear_v(value)  # Value: 실제 정보\n",
        "\n",
        "    # 2. QK^T 계산 및 스케일링\n",
        "    # K.transpose(-2, -1): 마지막 2개 차원을 바꿈\n",
        "    # (32, 20, 64) → (32, 64, 20)\n",
        "    # 결과: 각 위치가 다른 모든 위치와 얼마나 관련있는지 점수화\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "    # scores: (batch_size, seq_len, seq_len)\n",
        "\n",
        "    # 3. 마스크 적용 (선택사항)\n",
        "    # 디코더에서 미래 토큰 보지 않게 하려고 사용\n",
        "    # mask가 0인 곳을 -inf로 채워서 softmax에서 0이 되게 함\n",
        "    if mask is not None:\n",
        "      # masked_fill: 조건에 맞는 곳을 특정 값으로 채우기\n",
        "      # mask == 0인 곳을 -inf로 채움\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    # 4. Softmax로 주의 가중치 계산\n",
        "    # dim=-1: 마지막 차원(각 위치별로)에 대해 softmax\n",
        "    attention_weights = torch.softmax(scores, dim=-1)\n",
        "    attention_weights = torch.nan_to_num(attention_weights, 0.0)\n",
        "\n",
        "    # 5. Value와 곱해서 최종 출력\n",
        "    attention_output = torch.matmul(attention_weights, V)\n",
        "    # attention_output: (batch_size, seq_len, d_k)\n",
        "\n",
        "    return attention_output, attention_weights"
      ],
      "metadata": {
        "id": "3SQHd9erFgdt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Multi-Head Attention**"
      ],
      "metadata": {
        "id": "EH7opl0nPMux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 여러 개의 Self-Attention을 병렬로 실행\n",
        "    \n",
        "  #### **개념**:\n",
        "  - 1개 헤드: 모든 정보를 256차원에 집중\n",
        "  - 8개 헤드: 256차원을 8개 그룹(각 32차원)으로 나눠서\n",
        "  각각 다른 \"관점\"에서 주목\n",
        "    \n",
        "  #### **예시**:\n",
        "  \"I like apples\"을 분석할 때:\n",
        "  - 헤드1: 문법 구조에 집중 (I=주어, like=동사)\n",
        "  - 헤드2: 감정에 집중 (like=긍정, apples=좋은 것)\n",
        "  - 헤드3: 의존성에 집중 (like←I, apples←like)\n",
        "\n",
        "\n",
        "> 이렇게 다양한 관점의 정보를 모아서 더 풍부한 표현 생성"
      ],
      "metadata": {
        "id": "mKroOL6JPagP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads=8):\n",
        "    super().__init__()\n",
        "\n",
        "    # d_model이 num_heads로 나누어떨어지는지 확인\n",
        "    # 예: d_model=256, num_heads=8 → 256/8=32 (OK)\n",
        "    # 예: d_model=256, num_heads=7 → 256/7=36.57 (오류)\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    self.d_model = d_model  # 전체 임베딩 차원 (256)\n",
        "    self.num_heads = num_heads  # 헤드 개수 (8)\n",
        "    # 각 헤드의 차원: 256 / 8 = 32\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    # num_heads개의 Self-Attention 레이어 생성\n",
        "    # nn.ModuleList: 여러 모듈을 리스트로 관리\n",
        "    # 각 헤드는 d_model 입력을 d_k 출력으로 변환\n",
        "    # 예: 8개의 SelfAttention(256, 32) 생성\n",
        "    self.attention_heads = nn.ModuleList(\n",
        "        [SelfAttention(d_model, self.d_k) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "    # 모든 헤드의 출력을 연결한 후 선형변환\n",
        "    # 입력: concat된 모든 헤드 출력 (d_k * num_heads = d_model)\n",
        "    # 출력: d_model\n",
        "    # 예: (32, 20, 256) → (32, 20, 256) (형태는 같지만 가중치 재학습)\n",
        "    self.linear_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    # 1. 각 헤드별로 Self-Attention 실행\n",
        "    head_outputs = []  # 각 헤드의 출력을 저장할 리스트\n",
        "    all_attention_weights = []  # 각 헤드의 attention weight\n",
        "\n",
        "    # 8개 헤드 모두 실행\n",
        "    for attention_head in self.attention_heads:\n",
        "        # attention_head: SelfAttention 객체\n",
        "        # 반환: (attention_output, attention_weights)\n",
        "        # attention_output: (32, 20, 32) - 각 헤드는 32차원 출력\n",
        "        head_out, att_weights = attention_head(query, key, value, mask)\n",
        "\n",
        "        # 출력 저장\n",
        "        head_outputs.append(head_out)\n",
        "        all_attention_weights.append(att_weights)\n",
        "\n",
        "    # 2. 모든 헤드의 출력 연결 (concatenate)\n",
        "    # torch.cat(list, dim=-1): 마지막 차원에서 연결\n",
        "    concat_output = torch.cat(head_outputs, dim=-1)\n",
        "\n",
        "    # 3. 선형변환으로 최종 출력 생성\n",
        "    # (32, 20, 256) 을 다시 (32, 20, 256)으로 변환하지만\n",
        "    # 학습 가능한 가중치를 통해 헤드들의 정보를 종합\n",
        "    output = self.linear_out(concat_output)\n",
        "\n",
        "    return output, all_attention_weights"
      ],
      "metadata": {
        "id": "Q_IH9wshPY9v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Feed-Forward Network**"
      ],
      "metadata": {
        "id": "9XWq6BS6RnH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 각 토큰에 독립적으로 적용되는 완전 연결 네트워크\n",
        "    \n",
        "  #### **구조**:\n",
        "  - Linear(d_model -> d_ff) -> ReLU -> Linear(d_ff -> d_model)\n",
        "  \n",
        "  #### **예시**:\n",
        "  - 입력: (32, 20, 256) - 각 토큰의 256차원 벡터\n",
        "  - 확장: (32, 20, 2048) - 더 복잡한 연산을 위해 차원 확장\n",
        "  - ReLU: 음수를 0으로 만들어 비선형성 추가\n",
        "  - 축소: (32, 20, 256) - 다시 원래 차원으로\n",
        "  \n",
        "  #### **목적**:\n",
        "  - Attention에서는 관계를 학습\n",
        "  - FFN에서는 관계를 바탕으로 깊이 있는 표현 생성"
      ],
      "metadata": {
        "id": "7Q0x-JwyR7Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_ff=2048):\n",
        "      super().__init__()  # 부모 클래스 초기화\n",
        "      # 첫 번째 선형층: 차원 확장\n",
        "      # d_model=256 → d_ff=2048로 확장\n",
        "      # 예: (32, 20, 256) → (32, 20, 2048)\n",
        "      self.fc1 = nn.Linear(d_model, d_ff)\n",
        "      # 두 번째 선형층: 차원 축소\n",
        "      # d_ff=2048 → d_model=256으로 축소\n",
        "      # 예: (32, 20, 2048) → (32, 20, 256)\n",
        "      self.fc2 = nn.Linear(d_ff, d_model)\n",
        "      # ReLU 활성화 함수: max(0, x)\n",
        "      # 음수는 0, 양수는 그대로\n",
        "      # 비선형성을 추가해서 모델이 더 복잡한 함수 학습 가능\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 단계 1: 선형 변환으로 차원 확장\n",
        "    # (32, 20, 256) → (32, 20, 2048)\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    # 단계 2: ReLU 활성화\n",
        "    # 모든 음수를 0으로 변환\n",
        "    x = self.relu(x)\n",
        "\n",
        "    # 단계 3: 선형 변환으로 차원 축소\n",
        "    # (32, 20, 2048) → (32, 20, 256)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "gw0LWy6lRudk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Transformer Encoder Layer**"
      ],
      "metadata": {
        "id": "VlBYlWEsS_GK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer 인코더 레이어\n",
        "    \n",
        "  #### **구조**:\n",
        "  1. Multi-Head Attention\n",
        "  2. Add & Norm (잔여 연결 + 정규화)\n",
        "  3. Feed-Forward Network\n",
        "  4. Add & Norm\n",
        "  \n",
        "  #### **시각화**:\n",
        "  ```\n",
        "  입력 x\n",
        "    ↓\n",
        "  [Multi-Head Attention]\n",
        "    ↓ (attention 출력)\n",
        "  [Add & Norm]: x + attention_out, LayerNorm\n",
        "    ↓\n",
        "  [Feed-Forward]\n",
        "    ↓ (ffn 출력)\n",
        "  [Add & Norm]: 위_출력 + ffn_out, LayerNorm\n",
        "    ↓\n",
        "  출력\n",
        "  ```\n",
        "\n",
        "  #### **목적**:\n",
        "  - **Attention**: 토큰들 간의 관계 학습\n",
        "  - **FFN**: 각 토큰의 표현을 깊이 있게 변환\n",
        "  - **잔여 연결**: 깊은 네트워크에서도 정보 손실 방지\n",
        "  - **Layer Norm**: 훈련 안정화"
      ],
      "metadata": {
        "id": "vvxIAZltTGnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads=8, d_ff=2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Multi-Head Attention 모듈\n",
        "    # d_model=256, num_heads=8 → 각 헤드 32차원\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    # Feed-Forward Network 모듈\n",
        "    # 차원: d_model(256) → d_ff(2048) → d_model(256)\n",
        "    self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "\n",
        "    # Layer Normalization: 평균을 0, 표준편차를 1로 정규화\n",
        "    # 입력 d_model 차원에 대해 정규화\n",
        "    # 예: (32, 20, 256)을 마지막 차원(256)에 대해 정규화\n",
        "    self.norm1 = nn.LayerNorm(d_model)  # Attention 후 정규화\n",
        "    self.norm2 = nn.LayerNorm(d_model)  # FFN 후 정규화\n",
        "\n",
        "    # Dropout: 훈련 중 일부 뉴런을 무작위로 꺼서 과적합 방지\n",
        "    # dropout=0.1 → 10% 확률로 끔\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "\n",
        "    # 단계 1: Multi-Head Attention + 잔여 연결 + 정규화\n",
        "\n",
        "    # Multi-Head Attention 실행\n",
        "    # 입력 x를 query, key, value로 사용 (self-attention)\n",
        "    # 반환: (attention_output, attention_weights)\n",
        "    mha_output, _ = self.mha(x, x, x, mask)\n",
        "    # mha_output: (32, 20, 256)\n",
        "    # _: attention_weights는 여기서 사용 안 함\n",
        "\n",
        "    # Dropout 적용\n",
        "    # 훈련 중 10% 확률로 일부 값을 0으로 만듦\n",
        "    mha_output = self.dropout(mha_output)\n",
        "\n",
        "    # 잔여 연결 (Residual Connection)\n",
        "    # x + mha_output: 원래 입력에 attention 출력 더하기\n",
        "    # 이렇게 하면 정보가 손실되지 않음\n",
        "    x = x + mha_output\n",
        "\n",
        "    # Layer Normalization\n",
        "    # 평균 0, 표준편차 1로 정규화\n",
        "    # 모델 훈련이 더 안정적, 빨리 수렴\n",
        "    x = self.norm1(x)\n",
        "\n",
        "    # 단계 2: Feed-Forward Network + 잔여 연결 + 정규화\n",
        "\n",
        "    # FFN 실행\n",
        "    # 이전 단계의 출력을 입력으로 사용\n",
        "    ffn_output = self.ffn(x)\n",
        "    # ffn_output: (32, 20, 256)\n",
        "\n",
        "    # Dropout 적용\n",
        "    ffn_output = self.dropout(ffn_output)\n",
        "\n",
        "    # 잔여 연결\n",
        "    # x + ffn_output: 원래 x에 ffn 출력 더하기\n",
        "    x = x + ffn_output\n",
        "\n",
        "    # Layer Normalization\n",
        "    x = self.norm2(x)\n",
        "\n",
        "    # 최종 출력: (32, 20, 256)\n",
        "    return x"
      ],
      "metadata": {
        "id": "UIYxw9DUTSaf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Transformer Encoder**"
      ],
      "metadata": {
        "id": "AxhuDh74UsFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 여러 개의 Transformer 인코더 레이어를 쌓음\n",
        "    \n",
        "  #### **구조**:\n",
        "  ```\n",
        "  입력 시퀀스\n",
        "      ↓\n",
        "    [Embedding + Positional Encoding]\n",
        "      ↓\n",
        "    [인코더 레이어 1: Attention + FFN]\n",
        "      ↓\n",
        "    [인코더 레이어 2: Attention + FFN]\n",
        "      ↓\n",
        "    ...\n",
        "      ↓\n",
        "    [인코더 레이어 6: Attention + FFN]\n",
        "      ↓\n",
        "    출력 (각 토큰의 풍부한 표현)\n",
        "    \n",
        "    각 레이어마다 점점 더 복잡하고 추상적인 표현으로 변환됨\n",
        "  ```\n",
        "  "
      ],
      "metadata": {
        "id": "RRcbBHWiU2Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, d_model, num_layers=6, num_heads=8, d_ff=2048,\n",
        "                 max_seq_len=512, dropout=0.1):\n",
        "    super().__init__()  # 부모 클래스 초기화\n",
        "\n",
        "    # 위치 인코딩 모듈\n",
        "    # 시퀀스의 위치 정보를 추가\n",
        "    # max_seq_len=512: 최대 512 길이 시퀀스까지 지원\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "    # num_layers개의 인코더 레이어 생성\n",
        "    # nn.ModuleList: 여러 모듈을 리스트로 관리\n",
        "    # 예: 6개의 TransformerEncoderLayer"
      ],
      "metadata": {
        "id": "MQjNzB3pUzZ8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **미니 실험: 문장 감정 분류**"
      ],
      "metadata": {
        "id": "yUCdI93gDcpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실험 모델 조립 (Transformer Classifier)"
      ],
      "metadata": {
        "id": "lIqTkya6FQdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, num_classes, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # 1. 임베딩 및 위치 인코딩\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # 2. Transformer Encoder Layers를 리스트로 쌓음\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # 3. 분류를 위한 최종 출력층\n",
        "        # 전체 문장의 특징을 하나로 합치기 위해 평균(Mean Pooling)을 사용하거나\n",
        "        # 첫 번째 토큰([CLS])을 사용합니다. 여기서는 평균을 사용합니다.\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x: (batch_size, seq_len)\n",
        "        out = self.embedding(x) # (batch_size, seq_len, d_model)\n",
        "        out = self.pos_encoding(out)\n",
        "\n",
        "        # 여러 개의 Encoder Layer를 통과\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask)\n",
        "\n",
        "        # Global Average Pooling: 문장 전체 토큰의 벡터를 평균내어 문장 벡터 생성\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, d_model)\n",
        "        out = out.mean(dim=1)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        return self.fc(out) # (batch_size, num_classes)"
      ],
      "metadata": {
        "id": "PXb3VnbTE9nO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 영화 리뷰 감정 분류"
      ],
      "metadata": {
        "id": "lmyB1d95FF-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플 데이터셋\n",
        "data = [\n",
        "    (\"i love this movie\", 1),\n",
        "    (\"this film is great\", 1),\n",
        "    (\"i hate this movie\", 0),\n",
        "    (\"this film is terrible\", 0)\n",
        "]\n",
        "\n",
        "# 단어 사전 구축 및 정수 인코딩\n",
        "word_list = \" \".join([d[0] for d in data]).split()\n",
        "vocab = {\"<PAD>\": 0}\n",
        "for word in set(word_list):\n",
        "    vocab[word] = len(vocab)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "max_len = 5 # 모든 문장의 길이를 5로 맞춤\n",
        "\n",
        "def encode(text):\n",
        "    encoded = [vocab[w] for w in text.split()]\n",
        "    # 길이를 max_len에 맞게 패딩(0)\n",
        "    return encoded + [0] * (max_len - len(encoded))\n",
        "\n",
        "# 학습 데이터 텐서 변환\n",
        "input_batch = torch.tensor([encode(d[0]) for d in data]) # (4, 5)\n",
        "target_batch = torch.tensor([d[1] for d in data])        # (4)"
      ],
      "metadata": {
        "id": "Bv8UFNClFZx8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화\n",
        "model = TransformerClassifier(vocab_size, d_model=128, num_heads=4, d_ff=512,\n",
        "                              num_layers=2, num_classes=2, max_len=max_len)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 시작\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_batch)\n",
        "    loss = criterion(outputs, target_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 결과 해석\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_text = \"i love film\" # 학습에 없던 조합\n",
        "    test_input = torch.tensor([encode(test_text)])\n",
        "    prediction = model(test_input)\n",
        "    pred_class = torch.argmax(prediction, dim=1).item()\n",
        "\n",
        "    sentiment = \"긍정\" if pred_class == 1 else \"부정\"\n",
        "    print(f\"\\n테스트 문장: '{test_text}'\")\n",
        "    print(f\"모델의 예측 결과: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtN9wtyQGP6h",
        "outputId": "bd13cdb6-10f0-45db-d876-ab8f56b460ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 0.0014\n",
            "Epoch [40/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "\n",
            "테스트 문장: 'i love film'\n",
            "모델의 예측 결과: 긍정\n"
          ]
        }
      ]
    }
  ]
}